# ETL Workflow for Scalable Data Transformation  

This project showcases an **automated ETL pipeline** built with **Azure Data Factory, Databricks (PySpark/Python), and Azure Data Lake Storage**. It demonstrates how to ingest, transform, and store data in a **scalable and reliable** manner.  

---

## 🚀 Project Overview  
- **Extract**: Ingested multiple CSV files from GitHub using Azure Data Factory.  
- **Transform**: Applied data cleansing, joins, and aggregations in Databricks with PySpark/Python.  
- **Load**: Stored the processed data in Azure Data Lake Storage.  
- **Automation**: Scheduled pipeline runs using ADF triggers.  

---

## 🛠️ Tools & Technologies  
- Azure Data Factory (ADF)  
- Azure Databricks (PySpark, Python)  
- Azure Data Lake Storage  

---

## 📊 Key Features  
- Automated & scheduled pipeline runs  
- Scalable data processing with PySpark  
- Clean, integrated datasets ready for downstream use  
